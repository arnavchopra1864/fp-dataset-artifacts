{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjsonlines\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wordnet\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "# for some reason the notebook can't find nltk or spacy, use the .py file\n",
    "import jsonlines\n",
    "import random\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'snli_1.0/snli_1.0_dev.jsonl'\n",
    "\n",
    "data = []\n",
    "with jsonlines.open(file_path) as reader:\n",
    "    for obj in reader:\n",
    "        data.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_contrast_set(example):\n",
    "    tokens = nltk.word_tokenize(example['hypothesis'])\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    \n",
    "    contrasts = []\n",
    "    \n",
    "    for i, (word, pos) in enumerate(pos_tags):\n",
    "        # targeting verbs, adverbs, and adjectives\n",
    "        if pos.startswith(('VB', 'RB', 'JJ')):\n",
    "            synsets = wordnet.synsets(word)\n",
    "            for syn in synsets:\n",
    "                for lemma in syn.lemmas():\n",
    "                    if lemma.name() != word:\n",
    "                        new_tokens = tokens.copy()\n",
    "                        new_tokens[i] = lemma.name()\n",
    "                        contrasts.append({\n",
    "                            'premise': example['premise'],\n",
    "                            'hypothesis': ' '.join(new_tokens),\n",
    "                            'label': example['label']\n",
    "                        })\n",
    "    \n",
    "    return contrasts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spacy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_contrast_example\u001b[39m(sentence, print_debug\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m      4\u001b[0m     doc \u001b[38;5;241m=\u001b[39m nlp(sentence)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spacy' is not defined"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def create_contrast_example(sentence, print_debug=False):\n",
    "    doc = nlp(sentence)\n",
    "    new_sentence = sentence\n",
    "    modifications = []\n",
    "    \n",
    "    content_pos = ['NOUN', 'VERB', 'ADJ', 'ADV']\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.pos_ in content_pos:\n",
    "            synsets = wordnet.synsets(token.text)\n",
    "            alternatives = []\n",
    "            \n",
    "            for syn in synsets:\n",
    "                for lemma in syn.lemmas():\n",
    "                    if lemma.name().lower() != token.text.lower():\n",
    "                        alternatives.append(lemma.name())\n",
    "            \n",
    "            if alternatives:\n",
    "                replacement = random.choice(alternatives)\n",
    "                old_word = token.text\n",
    "                new_sentence = new_sentence.replace(old_word, replacement)\n",
    "                modifications.append((old_word, replacement))\n",
    "    \n",
    "    if print_debug and modifications:\n",
    "        print(\"\\nOriginal:\", sentence)\n",
    "        print(\"Modified:\", new_sentence)\n",
    "        print(\"Changes:\", modifications)\n",
    "        \n",
    "    return new_sentence, modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_contrast_dataset(data, sample_rate=0.1):\n",
    "    contrast_set = []\n",
    "    \n",
    "    for idx, example in enumerate(data):\n",
    "        original_premise = example['sentence1']\n",
    "        original_hypothesis = example['sentence2']\n",
    "        \n",
    "        # so modifying premise functionality can be added\n",
    "        modify_choice = random.choice(['hypothesis'])\n",
    "        \n",
    "        new_premise = original_premise\n",
    "        new_hypothesis = original_hypothesis\n",
    "        \n",
    "        should_print = random.random() < sample_rate\n",
    "        \n",
    "        # if modify_choice in ['premise', 'both']:\n",
    "        #     new_premise, premise_mods = create_contrast_example(\n",
    "        #         original_premise, \n",
    "        #         print_debug=should_print\n",
    "        #     )\n",
    "            \n",
    "        if modify_choice in ['hypothesis', 'both']:\n",
    "            new_hypothesis, hypothesis_mods = create_contrast_example(\n",
    "                original_hypothesis, \n",
    "                print_debug=should_print\n",
    "            )\n",
    "        \n",
    "        contrast_example = {\n",
    "            'original_premise': original_premise,\n",
    "            'original_hypothesis': original_hypothesis,\n",
    "            'contrast_premise': new_premise,\n",
    "            'contrast_hypothesis': new_hypothesis,\n",
    "            'gold_label': example['gold_label']\n",
    "        }\n",
    "        \n",
    "        contrast_set.append(contrast_example)\n",
    "        \n",
    "    return contrast_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print 10% of new examples\n",
    "contrast_dataset = create_contrast_dataset(data, sample_rate=0.1)\n",
    "\n",
    "output_path = 'snli_contrast_set.jsonl'\n",
    "with jsonlines.open(output_path, mode='w') as writer:\n",
    "    writer.write_all(contrast_dataset)\n",
    "\n",
    "# stats\n",
    "print(f\"\\nTotal examples processed: {len(data)}\")\n",
    "print(f\"Contrast examples created: {len(contrast_dataset)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
